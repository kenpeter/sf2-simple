# Qwen Street Fighter Agent

__win rate at 63.2%__

<img width="516" height="570" alt="win" src="https://github.com/user-attachments/assets/59fe8e24-bafc-40d7-b628-7c8bd63aba12" />


[training_small.webm](https://github.com/user-attachments/assets/f2f6f763-b9a5-4aeb-8150-f2e86ebe5a55)

## How the Qwen Agent Code Works - Flow Diagram

```
START
  â”‚
  â”œâ”€â”€â”€ INITIALIZATION PHASE
  â”‚     â”‚
  â”‚     â”œâ”€ Load Qwen2.5-VL vision model from cache
  â”‚     â”œâ”€ Setup 44 total actions (but focus on 7 basic ones)
  â”‚     â”œâ”€ Initialize cooldown timers and action history
  â”‚     â””â”€ Ready to play
  â”‚
  â”œâ”€â”€â”€ MAIN GAME LOOP
  â”‚     â”‚
  â”‚     â”œâ”€ Get game observation (screen frame + game state)
  â”‚     â”‚
  â”‚     â”œâ”€â”€â”€ TIMING CONTROL SYSTEM
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ Check if frame_counter % 30 == 0 (every 0.5 seconds)
  â”‚     â”‚     â”œâ”€ Check if action_cooldown <= 0 (not in recovery)
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ IF YES: Make new decision
  â”‚     â”‚     â””â”€ IF NO: Use cached action or NO_ACTION
  â”‚     â”‚
  â”‚     â”œâ”€â”€â”€ DECISION MAKING PROCESS (when allowed)
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ capture_game_frame()
  â”‚     â”‚     â”‚   â”œâ”€ Convert numpy observation to PIL Image  
  â”‚     â”‚     â”‚   â””â”€ Handle different image formats
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ extract_game_features()
  â”‚     â”‚     â”‚   â”œâ”€ Parse HP, positions, status from game info
  â”‚     â”‚     â”‚   â”œâ”€ Calculate distance, HP advantage, facing direction
  â”‚     â”‚     â”‚   â””â”€ Build strategic context
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ create_unified_prompt()
  â”‚     â”‚     â”‚   â”œâ”€ Analyze current situation (health, distance, positioning)
  â”‚     â”‚     â”‚   â”œâ”€ Add frame history context (HP changes, movement)
  â”‚     â”‚     â”‚   â”œâ”€ Create strategy based on distance:
  â”‚     â”‚     â”‚   â”‚   â”œâ”€ Close (<40px): Punch/Kick/Block
  â”‚     â”‚     â”‚   â”‚   â”œâ”€ Medium (<80px): Move closer
  â”‚     â”‚     â”‚   â”‚   â””â”€ Far (>80px): Move forward or jump
  â”‚     â”‚     â”‚   â””â”€ Generate action prompt focusing on basic moves only
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ query_qwen_vl()
  â”‚     â”‚     â”‚   â”œâ”€ Format messages with image + text prompt
  â”‚     â”‚     â”‚   â”œâ”€ Process inputs through Qwen2.5-VL model
  â”‚     â”‚     â”‚   â”œâ”€ Generate response (max 50 tokens, greedy decoding)
  â”‚     â”‚     â”‚   â””â”€ Return AI's text response
  â”‚     â”‚     â”‚
  â”‚     â”‚     â””â”€ parse_action_from_response()
  â”‚     â”‚         â”‚
  â”‚     â”‚         â”œâ”€ Extract numbers from AI response
  â”‚     â”‚         â”œâ”€ Prioritize basic actions (0,1,2,3,6,9,21)
  â”‚     â”‚         â”œâ”€ Convert complex actions to basic equivalents
  â”‚     â”‚         â”œâ”€ Try keyword matching if no numbers found
  â”‚     â”‚         â””â”€ Use cycling fallback as last resort
  â”‚     â”‚
  â”‚     â”œâ”€â”€â”€ ACTION PROCESSING
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ Anti-repeat system: prevent same action >2 times
  â”‚     â”‚     â”œâ”€ Set cooldown based on action recovery frames
  â”‚     â”‚     â”œâ”€ Cache action and reasoning for future frames  
  â”‚     â”‚     â””â”€ Update action history
  â”‚     â”‚
  â”‚     â”œâ”€â”€â”€ EXECUTE ACTION
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ Send action number (0-43) to game environment
  â”‚     â”‚     â”œâ”€ Environment processes action and updates game state
  â”‚     â”‚     â””â”€ Get reward/penalty based on combat effectiveness
  â”‚     â”‚
  â”‚     â””â”€ COOLDOWN MANAGEMENT
  â”‚           â”‚
  â”‚           â”œâ”€ Decrement action_cooldown counter each frame
  â”‚           â”œâ”€ Track frames_since_last_action
  â”‚           â””â”€ Block new decisions until cooldown expires
  â”‚
  â””â”€â”€â”€ EPISODE END
        â”‚
        â”œâ”€ Reset all counters and history
        â”œâ”€ Clear frame buffers and cached actions
        â””â”€ Ready for new episode

BASIC ACTIONS FOCUSED ON:
â”œâ”€ 0 = NO_ACTION (wait/rest)
â”œâ”€ 1 = JUMP (up movement)
â”œâ”€ 2 = CROUCH (down/low block)
â”œâ”€ 3 = LEFT (move left/block away)
â”œâ”€ 6 = RIGHT (move right/block toward)  
â”œâ”€ 9 = PUNCH (basic attack)
â””â”€ 21 = KICK (basic attack)

TIMING SYSTEM:
â”œâ”€ New decisions: Every 30 frames (0.5 seconds)
â”œâ”€ Action cooldowns: Vary by move (punch=11f, kick=12f, jump=15f)
â”œâ”€ Recovery frames: Must wait before next action
â””â”€ Real-time play: 60fps game, 2 decisions per second max
```



 # Data Collection:
  # Initial collection
  python lora_train.py --collect-data --episodes 1 --no-train

  # Append more data  
  python lora_train.py --collect-data --episodes 20 --append-data --no-train

  # Training:
  # Fresh training
  python lora_train.py --data-path ./data/sf2_training_data.json --epochs 1 --batch-size 1

  # With checkpoints (every 100 steps)
  python lora_train.py --data-path ./data/sf2_training_data.json --epochs 3 --save-steps 100

  # Resume training from checkpoint
  python lora_train.py --resume-from ./sf2_lora_model/checkpoint-6 --epochs 1 --batch-size 1

  # Inference:
  # Base model only
  python play.py --episodes 1 --no-render --quiet

  # With latest checkpoint (final model)
  python play.py --episodes 1 --model /home/kenpeter/.cache/huggingface/hub/Qwen2.5-VL-3B-Instruct-AWQ --lora ./sf2_lora_model

  # With specific checkpoint
  python play.py --episodes 1 --model /home/kenpeter/.cache/huggingface/hub/Qwen2.5-VL-3B-Instruct-AWQ --lora
  ./sf2_lora_model/checkpoint-6

  Now you can delete the sf2_lora_inference directory since it's no longer needed! ğŸ—‘ï¸

  



     ================================================================================
     â€¦ +82 lines (ctrl+r to expand)
  â¿  Found 1 new diagnostic issue in 1 file (ctrl-r to expand)

â— Perfect! ğŸ¯ Now you have all the direct download URLs. Here's your manual download strategy:

  ğŸš€ Manual Download (One file at a time):

  Step 1: Create directory

  mkdir -p ~/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Instruct-2507/snapshots/main/
  cd ~/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Instruct-2507/snapshots/main/

  Step 2: Download small config files (fast)

  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/.gitattributes'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/LICENSE'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/README.md'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/config.json'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/generation_config.json'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/merges.txt'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/model.safetensors.index.json'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/tokenizer.json'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/tokenizer_config.json'
  wget 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/vocab.json'
  # ... and the rest of the config files

  Step 3: Download BIG files ONE AT A TIME

  # Download first model file (3.96GB)
  wget -c 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/model-00001-of-00003.safetensors'

  # Then second model file (3.99GB)  
  wget -c 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/model-00002-of-00003.safetensors'

  # Finally third model file (99.6MB)
  wget -c 'https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/model-00003-of-00003.safetensors'

  Step 4: Test

  python demo_qwen.py --episodes 3

  ğŸ’¡ Benefits:

  - âœ… One file at a time - no parallel downloads
  - âœ… Resume support - wget -c to resume if interrupted
  - âœ… Full bandwidth to single file
  - âœ… Clear progress - you see exactly what's downloading

  This way you get complete control over the download process!




  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/config.json
  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/generation_config.json
  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/preprocessor_config.json
  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/chat_template.json


  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/added_tokens.json
  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/merges.txt
  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/tokenizer.json
  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/tokenizer_config.json
  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/vocab.json

  wget https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/model.safetensors






â— wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/config.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/generation_config.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model.safetensors.index.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/tokenizer.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/tokenizer_config.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/vocab.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/merges.txt
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/special_tokens_map.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/added_tokens.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/preprocessor_config.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/chat_template.json
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00001-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00002-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00003-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00004-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00005-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00006-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00007-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00008-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00009-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00010-of-00011.safetensors
  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ/resolve/main/model-00011-of-00011.safetensors




  # Create directory structure
  mkdir -p ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/main/
  cd ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/main/

  # Download all files
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/.gitattributes
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/README.md
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/chat_template.json
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/config.json
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/generation_config.json
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/merges.txt
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00001-of-00005.safetensors
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00002-of-00005.safetensors
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00003-of-00005.safetensors
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00004-of-00005.safetensors
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00005-of-00005.safetensors
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model.safetensors.index.json
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/preprocessor_config.json
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/tokenizer.json
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/tokenizer_config.json
  wget https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/vocab.json






  wget -c https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/model-00001-of-00002.safetensors \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/model-00002-of-00002.safetensors \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/model.safetensors.index.json \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/config.json \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/generation_config.json \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/preprocessor_config.json \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/tokenizer.json \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/vocab.json \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/tokenizer_config.json \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/merges.txt \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/chat_template.json \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/README.md \
       https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/LICENSE



  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/model-00001-of-00002.safetensors
  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/model-00002-of-00002.safetensors
  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/model.safetensors.index.json

  
  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/config.json
  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/generation_config.json
  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/preprocessor_config.json

  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/tokenizer.json
  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/vocab.json
  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/tokenizer_config.json
  https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/merges.txt


python train.py --mode train --resume train/best_model_1420000.zip --total_timesteps 6000000

  ta.json 
{
    "info": {
        "enemy_character": {
            "address": 16745563,
            "type": "|u1"
        },
        "agent_hp": {
            "address": 16744514,
            "type": ">i2"
        },
        "agent_x": {
            "address": 16744454,
            "type": ">u2"
        },
        "agent_y": {
            "address": 16744458,
            "type": ">u2"
        },
        "enemy_hp": {
            "address": 16745154,
            "type": ">i2"
        },
        "enemy_x": {
            "address": 16745094,
            "type": ">u2"
        },
        "enemy_y": {
            "address": 16745098,
            "type": ">u2"
        },
        "score": {
            "address": 16744936,
            "type": ">d4"
        },
        "agent_victories": {
            "address": 16744922,
            "type": "|u1"
        },
        "enemy_victories": {
            "address": 16745559,
            "type": ">u4"
        },
        "round_countdown": {
            "address": 16750378,
            "type": ">u2"
        },
        "reset_countdown": {
            "address": 16744917,
            "type": "|u1"
        },
        "agent_status": {
            "address": 16744450,
            "type": ">u2"
        },
        "enemy_status": {
            "address": 16745090,
            "type": ">u2"
        }
    }




â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            GAME ENVIRONMENT                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Raw Frame: (H, W, 3) RGB                                                     â”‚
â”‚  Game State: health, position, score, status                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          PREPROCESSING STAGE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frame Resize: (128, 180, 3)                                                  â”‚
â”‚  Frame Stack: 8 frames â†’ (24, 128, 180) for CNN input                        â”‚
â”‚  Strategic Processing: Extract 21 strategic features                           â”‚
â”‚  Button History: Track 12 previous button states                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FEATURE EXTRACTION PIPELINE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  CNN EXTRACTOR  â”‚    â”‚ STRATEGIC TRACK â”‚    â”‚ BUTTON HISTORY  â”‚           â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚           â”‚
â”‚  â”‚ Input:          â”‚    â”‚ Input:          â”‚    â”‚ Input:          â”‚           â”‚
â”‚  â”‚ (24,128,180)    â”‚    â”‚ Game state info â”‚    â”‚ Previous action â”‚           â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚           â”‚
â”‚  â”‚ Output:         â”‚    â”‚ Output:         â”‚    â”‚ Output:         â”‚           â”‚
â”‚  â”‚ 512-dim vector  â”‚    â”‚ 21-dim vector   â”‚    â”‚ 12-dim vector   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TEMPORAL FEATURE MANAGEMENT                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  Visual History:    [8 Ã— 512] = (8, 512)                                     â”‚
â”‚  Strategic History: [8 Ã— 21]  = (8, 21)                                      â”‚
â”‚  Button History:    [8 Ã— 12]  = (8, 12)                                      â”‚
â”‚                                                                                â”‚
â”‚  Combined Sequence: (8, 545) = (8, 512+21+12)                                â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CROSS-ATTENTION VISION TRANSFORMER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    FEATURE GROUP PROCESSORS                             â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚  Visual:    (8, 512) â†’ Visual Processor    â†’ (8, 256)                 â”‚  â”‚
â”‚  â”‚  Strategy:  (8, 21)  â†’ Strategy Processor  â†’ (8, 256)                 â”‚  â”‚
â”‚  â”‚  Button:    (8, 12)  â†’ Button Processor    â†’ (8, 256)                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                                           â”‚
â”‚                                    â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    CROSS-ATTENTION LAYERS                               â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚  Learnable Query: "What button should I press now?" â†’ (1, 256)        â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚  Visual Cross-Attention:    Q=(1,256), K=V=(8,256) â†’ (1,256)         â”‚  â”‚
â”‚  â”‚  Strategy Cross-Attention:  Q=(1,256), K=V=(8,256) â†’ (1,256)         â”‚  â”‚
â”‚  â”‚  Button Cross-Attention:    Q=(1,256), K=V=(8,256) â†’ (1,256)         â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚  Multi-Head Attention: 8 heads per cross-attention layer              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                                           â”‚
â”‚                                    â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      FEATURE FUSION                                     â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚  Concatenate: (1,256) + (1,256) + (1,256) = (1,768)                  â”‚  â”‚
â”‚  â”‚  Fusion Network: (1,768) â†’ (1,256)                                    â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚  Temporal Attention: Query=(1,256), Key=Value=(8,768) â†’ (1,256)       â”‚  â”‚
â”‚  â”‚  Final Features: (1,256) â†’ squeeze â†’ (256)                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PPO POLICY NETWORK                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        POLICY BRANCH            â”‚    â”‚        VALUE BRANCH             â”‚   â”‚
â”‚  â”‚                                 â”‚    â”‚                                 â”‚   â”‚
â”‚  â”‚  Input: (256) processed featuresâ”‚    â”‚  Input: (256) processed featuresâ”‚   â”‚
â”‚  â”‚         â†“                       â”‚    â”‚         â†“                       â”‚   â”‚
â”‚  â”‚  FC Layer: 256 â†’ 512            â”‚    â”‚  FC Layer: 256 â†’ 512            â”‚   â”‚
â”‚  â”‚  ReLU + Dropout                 â”‚    â”‚  ReLU + Dropout                 â”‚   â”‚
â”‚  â”‚         â†“                       â”‚    â”‚         â†“                       â”‚   â”‚
â”‚  â”‚  FC Layer: 512 â†’ 256            â”‚    â”‚  FC Layer: 512 â†’ 256            â”‚   â”‚
â”‚  â”‚  ReLU + Dropout                 â”‚    â”‚  ReLU + Dropout                 â”‚   â”‚
â”‚  â”‚         â†“                       â”‚    â”‚         â†“                       â”‚   â”‚
â”‚  â”‚  Output: 57 action logits       â”‚    â”‚  Output: 1 value estimate       â”‚   â”‚
â”‚  â”‚  (Discrete action distribution) â”‚    â”‚  (State value V(s))             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ACTION SELECTION & EXECUTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  Action Sampling: Sample from 57-way categorical distribution                  â”‚
â”‚                   â†“                                                            â”‚
â”‚  Discrete Action: Integer index (0-56)                                        â”‚
â”‚                   â†“                                                            â”‚
â”‚  Action Converter: Map discrete index to button combination                    â”‚
â”‚                   â†“                                                            â”‚
â”‚  Multi-Binary: 12-dimensional binary vector                                   â”‚
â”‚                [B, Y, SELECT, START, UP, DOWN, LEFT, RIGHT, A, X, L, R]       â”‚
â”‚                   â†“                                                            â”‚
â”‚  Game Input: Execute button combination in game                               â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          TRAINING FEEDBACK LOOP                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  Reward Signal: Combat performance + combo bonuses                            â”‚
â”‚  PPO Loss: Policy loss + Value loss + Entropy bonus                          â”‚
â”‚  Gradient Update: Backprop through entire network                             â”‚
â”‚                   â†“                                                            â”‚
â”‚  Parameter Update: CNN â†’ Cross-Attention â†’ Policy/Value Networks              â”‚
â”‚                                                                                â”‚
